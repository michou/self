<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>From Jenkins apprentice to AWS master: running automation at scale</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/simple.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/atom-one-light.css">

		<script src="https://twemoji.maxcdn.com/2/twemoji.min.js?12.0.4" crossorigin="anonymous"></script>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
		<style>
		img.emoji {
			border: none !important;
			box-shadow: none !important;
			
			height: 1em;
			width: 1em;
			
			margin: 0 .05em 0 .1em !important;
			vertical-align: -0.1em;
		}
		.reveal p {
			text-align: left;
		}
		.reveal h1,
		.reveal h2,
		.reveal h3 {
			font-weight: 900;
		}
		.left-pad {
			padding-left: 1em !important;
		}

		p code {
			color: rgb(96, 96, 96);
		}

		span.dense {
			font-size: 80%;
			color: rgb(96, 96, 96);
		}

		.dense p,
		.dense li,
		.dense pre code {
			font-size: 90%;
		}

		.denser p,
		.denser li,
		.denser pre code {
			font-size: 80%;
		}
		.dimmer {
			opacity: 0.4 !important;
		}
		p.subpoint {
			padding-left: 1em;
			font-size: 90%;
			margin: 0.25em 0;
			color:darkslategray;
		}
		div.speaker-notes p {
			margin-top: 0;
			margin-bottom: 0.2em;
		}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1 style="font-size: 2em;">
						<span class="dimmer">From</span> Jenkins apprentice<br><span class="dimmer">to</span> AWS master
					</h1>
					<h2 style="font-size: 1.8em">Running automation at scale</h2>
					<p style="text-align: right;">&mdash; Mihai Balan, Adobe Identity/IMS</p>
					<aside class="notes" data-trim data-markdown>Hi, I'm Mihai Balan, I'm a software engineer with the Adobe Identity group, working on IMS ‚Äì the Identity Management System.
I've been with this team, this product, for 5+ years now, and I've seen in grow and expand in many ways.

So this talk will be about what this growth meant for testing IMS, and how we approached running automation at scale, inside IMS.
					</aside>
				</section>
				<section>
					<section>
						<h2>Key takeaways</h2>
						<aside class="notes" data-trim data-markdown>Since what follows is essentially a tale and it might end up rather long-winded, I thought about doing things a little backwards and start with (some) takeaways.
I'll have some more at the end, but in case I cause you to doze off, here's the 3 most important things. Admittedly, thiey might be highly personal, but I think the tale is interesting nonetheless.
						</aside>
					</section>
					<section>
						<h2>üôÄüî´</h2>
						<h3>Use forced change as an opportunity to do things right</h3>
						<div class="fragment">
							<p>Even if this means sneaking work in</p>
							<p>Try to take in and reassess the big picture</p>
							<p class="fragment subpoint">Unexpected things might end up requiring change</p>
						</div>
						<aside class="notes" data-trim data-markdown>Sometimes the proverbial poop hits the proverbial fan and you have to put everything on hold to fix whatever caused the breakage.
Usually, these moments are caused by compounded poor decisions along the way and highlight architectural or system weaknesses.

Use this opportunity to try to do things better ‚Äì architecturally, process-wise, organization-wise.
Sometimes it might even mean sneaking in work that won't be directly related to the root-cause at hand, but that will improve life for everybody nonetheless.

Cherish this _carte blanche_ and use the opportunity to figure out and reassess the big picture: maybe a multitude of minute changes resulted in a massive shift; maybe some strongly-held assumptions no longer hold; maybe the process is ripe for disruption.
If you manage to put things in perspective and see exactly what in the big picture needs to change, you might end up changing things you didn't expect to.

The prime example here is adopting Continuous Delivery: it's not only about the tools, it's also about the mindset, the culture and about a lot of supporting systems.
						</aside>
					</section>
					<section>
						<h2>‚òÅÔ∏èüèÜ</h2>
						<h3>If you go cloud, make the best of it</h3>
						<div class="fragment">
							<p>Don&rsquo;t roll your own</p>
							<p>Don&rsquo;t worry about vendor lock-in (just yet)</p>
						</div>
						<div class="fragment">
							<blockquote class="twitter-tweet tw-align-center" data-lang="en"><p lang="en" dir="ltr">so, you avoid vendor lock-in by building your own. and then later realize you still have vendor lock-in, but you&#39;re the vendor, and there are no other customers. oops.</p>&mdash; Alex Lambert (@alambert) <a href="https://twitter.com/alambert/status/958050456101642241?ref_src=twsrc%5Etfw">January 29, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
						</div>
						<aside class="notes" data-trim data-markdown>Another thing that we learned the hard way is that, if you decide to move to the cloud, it's best to spend as little time as possible redoing things that the provider already has on offer.

There's still plenty of interesting and pressing issues to figure out even without reinventing the wheel.
						</aside>
					</section>
					<section>
						<h2>üë∑üèªüí©</h2>
						<h3>Orchestrating <em>on-prem</em> *and*<br> <em>cloud-based</em> Jenkins is crappy</h3>
						<div class="fragment">
							<p>&hellip; but it can be done</p>
							<p>We did it with a mix of shared storage (AWS S3), polling (HTTP) and a lot of brittle conventions</p>
						</div>
						<aside class="notes" data-trim data-markdown>The final, most pragmatic and probably most personal takeaway is that orchestrating Jenkins instances across network boundaries sucks.

You can do it, it will work, but it won't get better with age. The only way to make things bearable? Treat infrastructure as code as soon as possible.
						</aside>
					</section>
				</section>
				<section>
					<h2>IMS at a glance</h2>
					<section>
						<p>Sign-in and sign-up</p>
						<p class="fragment">But so much more&hellip;</p>
						<p class="fragment subpoint">Federation</p>
						<p class="fragment subpoint">Account recovery</p>
						<p class="fragment subpoint">Security</p>
						<p class="fragment subpoint">APIs</p>
						<aside class="notes" data-trim data-markdown>With pretty much most of the wisdom of this talk behind us, let me give you some more context about what IMS _actually_ is, and how we are testing it now.

In a nutshell, the most important and visible things IMS is in charge of is sign in and sign up for all Adobe products.
And while this might sound like not that much, in fact there's a lot more things happening under the hood.

As far as direct user interaction is concerned we're dealing with federation, account recovery and security, but there's also a plethora of APIs that different Adobe services rely on in order to provide their own features.
						</aside>
					</section>
					<section>
						<h3>üçå for scale</h3>
						<p class="fragment">Serving +1M RPM</p>
						<p class="fragment">99.99% availability</p>
						<p class="fragment subpoint">52 minutes of downtime <strong>per year</strong></p>
						<p class="fragment">60+ people working on it</p>
						<aside class="notes" data-trim data-markdown>In terms of scale, IMS is currently serving 1+M RPM, sporting an SLA with 4 nines of availability. To put things in perspective, it means that on the whole, we can only have 52 minutes of downtime per year, including maintenance, incidents, attacks.

On the human scale, this means about 60 people working on this system, directly or indirectly.
And if you think that's a lot, the Identity group at Google has close 1000 people üòé
						</aside>
					</section>
					<section>
						<h3>üìà The growth</h3>
						<p class="fragment">Operational complexity and feature footprint</p>
						<p class="fragment subpoint"><span class="dimmer">from</span> custom UIs for each supported platform <span class="dimmer">to</span> one UI to rule them all <span class="dense fragment">(now at v3)</span></p>
						<p class="fragment subpoint">multiple federation and authorization models</p>
						<p class="fragment subpoint"><span class="dimmer">from</span> 4 components <span class="dimmer">to</span> 40+ components</p>
						<p class="fragment subpoint"><span class="dimmer">from</span> own datacenter <span class="dimmer">to</span> multi-cloud, multi-region</p>
					<aside class="notes" data-trim data-markdown>All the stats that I just gave describe the IMS of today. In the last 5-6 years it has grown however quite a lot to reach these numbers.
It was not only a growth in number of features, but also in the number of clients, customers and, accordingly, operational complexity.

We went from having multiple UIs for each supported platform: iPhones got an UI, desktop apps got an UI, different web apps got different UIs;
we went to have this single, unified UI that had to work the same on all these surfaces, and still had to have a lot of flexibility in terms ofbig and small configuration tweaks.

We went from only supporting username & password logins to allowing people to log in with their facebook, or Google, or Office365 LDAP account. We also added support for recovering your account via SMS, and 2FA.

In terms of the sheer number of deployed components, we went from having 4 components to building now more than 40 components, some shared between products and services.

And we moved from Adobe's US datacenter to AWS and Azure and IMS now has 4 regions in 2 different clouds.
					</aside>
					</section>
				</section>
				<section>
					<h2>Testing IMS</h2>
					<section>
						<h3>Validations</h3>
						<p>Mostly end-to-end integration tests, with <span class="fragment">(too few)</span> unit & functional tests</p>
						<p class="fragment subpoint">Feature-branch validation</p>
						<p class="fragment subpoint">Post-merge & nightly validation</p>
						<p class="fragment subpoint">Staging environment validation</p>
						<aside class="notes" data-trim data-markdown>To make sure we don't deploy broken code to our clients and users we have a tiered validation approach.
There's a lot of end-to-end (we call them integration) tests and, alas, too little unit & functional tests.
These tests run at different points in the lifecycle of a feature.

After writing the code but before merging you can validate your branch of the code. 

Once the code is merged, a new test run is triggered; and then every night some extra validations are run.

Finally, before pushing to code the staging environment everybody else in Adobe uses we run another battery of tests.
						</aside>
					</section>
					<section>
						<h3>Environments</h3>
						<p>We don&rsquo;t test in Prod <span class="dense fragment">(much)</span></p>
						<p class="fragment">Multiple levels of pre-Prod environments</p>
						<p class="fragment subpoint">Staging</p>
						<p class="fragment subpoint">Dev experimental</p>
						<p class="fragment subpoint">Continuous Integration (CI) validation</p>
						<aside class="notes" data-trim data-markdown>As you might already guess, we don't rely on just our Production environment for these validations.
In fact, we hardly even test in Production (whether this is a good or bad idea sounds like a nice topic for another presentation).
We instead rely on multiple levels of pre-Production environments, with more or less official SLAs.

We have staging environments that other teams in Adobe integrate against, both for development and testing.
We have a number of experimental dev environments that are used by sub-teams in IMS for developing more complex features.
And finally we have a number of Continuous Integration (CI) environments that are routinely used by pretty much everyone in the IMS team.
						</aside>
					</section>
					<section>
						<h3>üìà The growth</h3>
						<p class="fragment subpoint"><span class="dimmer">from</span> 1500 tests <span class="dimmer">to</span> 9000+ tests</p>
						<p class="fragment subpoint"><span class="dimmer">from</span> tests and code running on the same machine <span class="dimmer">to</span> cloud-based Jenkins slaves</p>
						<p class="fragment subpoint"><span class="dimmer">from</span> a lone mock mail server <span class="dimmer">to</span> multiple mocked & real external services</p>
						<p class="fragment subpoint"><span class="dimmer">from</span> jMeter <span class="dimmer">to</span> Soasta and Gatling</p>
						<aside class="notes" data-trim data-markdown>As I said about IMS, these stats reflect the current state of our testing infrastructure.
It too, had come a long way.

It started from scratch with no tests (a couple of years before I joined the team) and a fondness for TestNG. Back when I joined, it already had around 1500 tests. Now, we're nearing 10k

We went from having the product and the tests running on the same, manually configured machine, to having multiple CI environments deployed in the cloud, with the test runners automatically provisioned in AWS.

For some of the flows, like reading the emails the system was sending out, we had to build some support systems. We moved from having this very simple mock server email to also mocking Google and an exteral SMS provider (I've also given [a talk](https://michou.github.io/self/presentations/www/mocking-in-the-real-world.html) about this last year).

Finally, we gradfuated from doing load testing using jMeter on one tester's Mac to using Akamai's SOASTA Cloud Test to using Gatlin and finally having performance testing as code.
					</aside>
					</section>
				</section>
				<section>
					<h2>ü¶ï In the beginning&hellip;</h2>
					<aside class="notes" data-trim data-markdown>These being said, let's take the time machine and see how exactly these changes came to be
					</aside>
				</section>
				<section>
					<h3>Life was rather simple</h3>
					<p class="fragment">Java 1.4, 1400 tests in 150 test classes</p>
					<p class="fragment">Everything run on a manually configured, local machine</p>
					<p class="fragment">Some nifty tricks in place to exercise the same flows on different UIs</p>
					<p class="fragment">One test suite, run post-commit</p>
					<aside class="notes" data-trim data-markdown>Back when dinosaurs and Java 1.4 still roamed the Earth, ~6 years ago, life was rather simple.

The tests were written using Java 1.4, we had about 1400 tests in some 150 classes, and all our test plans were comments in those test classes.

Both the tests and the test environment run on the same machine ‚Äì a workstation tucked under one of the developer's desk. So basically if something hung, or the power went out, you could at best SSH to that machine and at worst crawl under said desk to plug in a keyboard and monitor and fix everything yourself.

At that time, the system had custom UIs for the different platforms, each with it's own slightly different look and feel, but all enabling pretty much the same user flows.
And some very smart people did some very smart things with TestNG that allowed us to write the test once and run it on all the UIs.

And all was good in the world, and we only had a single test suite, that run **after** the code was merged.
					</aside>
				</section>
				<section>
					<h2>üíÄ And then that machine died</h2>
					<aside class="notes" data-trim data-markdown>This was rather unpleasant since all the setup for the CI environment was long and manual and tedious.
This was caused by the fact that you had the setup both the environment for the actual product and for the test run.
					</aside>
				</section>
				<section>
					<h3>How do you avoid this?</h3>
					<p class="fragment">Migrate to VMs for test environments</p>
					<p class="fragment">Opportunity to upgrade to more powerful hardware</p>
					<p class="fragment">More slaves!</p>
					<p class="fragment subpoint">Code coverage</p>
					<p class="fragment subpoint">Feature-branch validation</p>
					<aside class="notes" data-trim data-markdown>So after we realized that one way or another we'll have to do it all over again, we asked ourselves, what can we do to make this the _last_ time we have to do it?
The obvious answer was to use Virtual Machines.

Where we previously had a "server" we managed ourselves (actually a workstation under one of the developer's desks), we moved to a dedicated VM server that was managed by the local IT support team.

This being a real server (run VMWare ESXi), we had better hardware so we decided to put said hardware to work:
we added separate slaves for building the code and running the tests, started doing code coverage and implemented feature-branch validation (essentially, validating your code before merging it)
					</aside>
				</section>
				<section>
					<h2>üßπ And then the VM infrastructure got consolidated</h2>
					<aside class="notes" data-trim data-markdown>It wasn't perfect, it wasn't all magic and fairy dust, but at least you didn't have to crawl under a desk anymore.
But one fateful day, it was decided (very _high up_) the VM infrastructure will be consolidated and offered as a service (think Adobe building it's own toy AWS inside the intranet)
					</aside>
				</section>
				<section>
					<h3>The tests we getting slower *anyway*</h3>
					<p class="fragment">Bugs were slipping in, too</p>
					<p class="fragment">Move everything to the cloud</p>
					<p class="fragment subpoint">Cloud-based CI environments</p>
					<p class="fragment subpoint">Cloud-based Jenkins slaves</p>
					<p class="fragment">But&hellip;</p>
					<p class="fragment subpoint">Source code is not allowed to leave the premises</p>
					<p class="fragment subpoint">Build Jenkins is invisible to the test Jenkins</p>
					<aside class="notes" data-trim data-markdown>Given the hard deadline and a mandate to move out of the soon-to-be-decommissioned local lab, we also confronted the harsh truth:
not only were the tests getting slower (by sheer virtue of number), but they were also on the critical path.
And to top it off, because of the rather big configuration mismatch between real and CI environments some bugs were only found in the "real" environments.

By that time, the "real" environment had already moved to AWS, so using the same stack to deploy the CI environments was an obvious choice.
Using AWS to run the tests as well also meant access to better hardware, which could help us reach the under-30-minutes-per-test-run sweet spot.

There were some problems however: the code wasn't allowed to leave the Intranet, so we had to build it inside the intranet. We then had to start the test on the Jenkins master that was in the cloud, but getting a secure tunnel from our intranet to AWS never happened so we had to make do with one-way communication between the Jenkins master.
					</aside>
				</section>
				<section>
					<h2>üê¢ With big products come big test suites</h2>
					<aside class="notes" data-trim data-markdown>The solution we ended up with for the test runners, involved an intranet Jenkins that build the code and an AWS-based Jenkins master, with a manually configured set of slaves on which docker was used to actually run the tests.
The intranet Jenkins would build the artifacts, upload them to S3 and then hit the AWS Jenkins' endpoint to start a job. It would then keep polling the AWS Jenkins for results until the tests were finished.

It was still clunky at times, and changing software dependencies was tedious, but using docker to configure the environment and run the tests sure beat SSH-ing and apt-get-ing on actual machines (virtual or otherwise).
					</aside>
				</section>
				<section>
					<h3>It was time to scale horizontally *anyway*</h3>
					<p class="fragment">Running all the tests on single machine took too long</p>
					<p class="fragment">Split tests in suites that can be run in parallel</p>
					<p class="fragment">Optimize build and deployment process</p>
					<aside class="notes" data-trim data-markdown>However, the AWS high didn't last long; we were reaching a point where the initial 8-cores, 32GB RAM machines weren't enough and even moving to 16-cores, 64GB RAM machines didn't improve things enough to justify the price difference.
This was caused by the fact that the tests themselves, were split in 2 test suites (rather arbitrarily) and test suite parallelization on a single machine just didn't scale well.

To put things in perspective, we were already using 3 CI environments: 2 for feature-branch validation and one for mainline validation and still the test suites were using more computing power than the CI environments they were hitting.

The easy way out was to throw even more machines at the problem, by splitting the tests into more test suites (again, manually) so we can run more tests in parallel.

Again, some rather smart people also managed to squeeze some precious minutes out of the build and deployment process and one sunny day in 2016 we were down to 12 minutes from code to test results
					</aside>
				</section>
				<section>
					<h2>üí∏ But then cost started to become an issue</h2>
					<aside class="notes" data-trim data-markdown>One of the shortcomings of this solution was that the Jenkins slaves were up and incurring costs 24/7, even when no tests were running.
Initially, for all things cloud, cost was not an issue, but as everything got migrated, management started to clamp down on costs.
					</aside>
				</section>
				<section>
					<h3>Upgrading the slaves was hard *anyway*</h3>
					<p class="fragment">Moved from docker-based, statically configured slaves to on-demand, ASG-based slaves</p>
					<p class="fragment">Time to start mocking some external services, too</p>
					<p class="fragment">But the test suites are still rather beefy</p>
					<aside class="notes" data-trim data-markdown>At the same time, we had some gripes of our own, namely the tediousness of changing the software configuration of the slaves, which involved changing a dockerfile in the code repo, committing the change and then making sure the change got propagated.
And this was the happy case where you knew exactly what needed changing, and how. In addition, since the Jenkins slaves were statically configured, adding a new slave was a matter of hours, as it had to be manually configured, at least up to a point.

In keeping with the configuration-as-code mantra, we decided to migrate to a on-demand system, where slaves would be spun up and down for each test run.
Technically, we ended up using AWS AutoScaling Groups that were using our custom, preconfigured image to boot.
The main benefit here was that changing the software configuration of all slaves was just a matter of creating a new image (usually based on the existing one) and then changing the ASG setting to use the new image.

Also around this time, the number of external dependencies of the actual system increased quite a bit.
Which also meant that the tests were hitting even more interconnected systems causing flakiness to increase considerably and performance to drop. Some systems were plain slow (facebook), while others would just not work (Twilio).

And while we mitigated some of these problems by mocking a number of external services (some of you might remember my talk on the subject from last year), we still have
massive test suites that take a non-negligible amount of time to run.
					</aside>
				</section>
				<section>
					<h2>üöÄ To boldy go where no one has gone before</h2>
					<aside class="notes" data-trim data-markdown>The good thing is, after a rather radical reorganization (more on which maybe some other time), we did start working on things I wouldn't have hoped to see done before 2020 üòÖ.
					</aside>
				</section>
				<section>
					<h3>The future is now</h3>
					<p class="fragment">Improved architecture for more efficient testing</p>
					<p class="fragment subpoint"><span class="dimmer">from</span> 85:15 <span class="dimmer">to</span> 5:95 ratio of integration to non-integration tests</p>
					<p class="fragment">Performance tests as code</p>
					<p class="fragment">On-demand CI environments</p>
					<p class="fragment">In-house test runner</p>
					<aside class="notes" data-trim data-markdown>As I mentioned at the beginning of the talk, we're in the process of rolling out a new and improved UI. This new system had the benefit of starting from scratch, which enabled us to reconsider and improve our building and testing in many ways:
- we have a radically more testable architecture. To give you a sense of scale, where in the "old" system about 15% of the tests were unit and functional and 85% were end-to-end integration test, for the new UI we have only about 100 end-to-end tests, and about 95% of all tests are unit and functional (component) tests
- we have more granular pipelines and we're one-step away from real CD ‚Äì right now we're only delivering up to stage
- we took our testing tooklit up a notch with Kotlin

Independent of the new UI rollout, we now handle performance testing as code, by moving from SOASTA to Gatling (we're also saving üí∞)

And we're also doing some really exciting work, enabling the old testing framework to run with on-demand CI environments and creating our own test runner to better scale horizontally.
					</aside>
				</section>
				<section>
					<h2>üìñ What came out of this?</h2>
					<aside class="notes" data-trim data-markdown>I know this was a long story, and I also divulged the key takeaways before hand, but let's dive a little deeper in what did we learn ‚Äì and might be useful for you as well.
					</aside>
				</section>
				<section>
					<h2>Lessons learned</h2>
					<section>
						<h3>üôÑ The bad</h3>
						<p class="fragment">Deleting code is harder than writing code</p>
						<p class="fragment">Scaling out only exposes the critical path</p>
						<p class="fragment">Tests performance profiling matters</p>
						<p class="fragment subpoint">Mind your pre/post-test code</p>
						<p class="fragment subpoint">Know thy framework</p>
						<aside class="notes" data-trim data-markdown>Some of these lessons, we learned them the hard way ‚Äì whether that meant long hours spent on trivial issues, handling deal-breaker performance bottlenecks or just diving deep into the guts of TestNG or Spring.

First and foremost, it turns out that oftentimes removing code is a lot harder than putting it in, and even moreso when it comes to test code and test infrastructure code.
We recently spent the better part of 2 months to remove from the testing framework the code that tested ‚Äì and relied on ‚Äì code that was deprecated in production more than a year ago.

As we came up with more and more tricks to reduce the time it took to run the tests, we found out that, at cases, the long run time was caused by just a handful of tests that took waay too long to complete ‚Äì think one test running for 10min.
This came with the realization that while people usually do care about the performance of the product they're building they're considerably less involved, and held accountable, for the performance of the tests they're writing.

Unfortunately, it's not always a matter of writing a better test and paying more attention to the performance implications.
We had our fair share of situations where the performance bottleneck required serious profiling and investigation.
In these cases the culprits were outside of the `@Test` code body, _per-se_, but were rather code that was executing before or after the test, either serially or with a lower degree of parallelization.
The key takeaway here is that, as you stretch the limits of the framework and infrastructure, a deep understanding of your toolchain becomes mandatory in order to keep things running smoothly.

As an example, just recently I spent the better part of a month hunting down a a problem that was ultimately caused by an initialization that was happening too late in the Spring lifecycle of the tests (the tests are using Spring tests support for TestNG).
						</aside>
					</section>
					<section>
						<h3>üòé The good</h3>
						<p class="fragment">Shared pain is easier acted on</p>
						<p class="fragment subpoint">Also, the testing pyramid is real</p>
						<p class="fragment">Don&rsquo;t be afraid to dream, but advocate for extra testability while you&rsquo;re at it</p>
						<p class="fragment">Don&rsquo;t be afraid to roll your own</p>
						<aside class="notes" data-trim data-markdown>But there were also some things that really made me happy and gave me hope.

In the early configurations of the teams, writing tests and making sure they were running properly, and fast, and passing was the sole responsibility of the testers.
But as more people started contributing and as developers started writing tests, people (developers, that is), started complaining about usability and performance issues with the testing framework.
Many of these problems were raised by testers along the way, but now they were blocking developers and this is what led to managers actually giving people the time to improve things at the testing level as well.

On the new version of the UI that we're working on now we were able to build-in more testability from the get go, and now we have a lot more tests at lower levels (unit & functional) and this proved really beneficial.
So even though it might seem obvious, or purely academic, the testing pyramid is real, and trying to have it in your testing is really helpful.

One of the really cool things that happened in just the last couple of months was that people started working on, and shipping, framework and infrastructure improvements that really help a lot.
And these are things 6-9 months ago I wouldn't even have dreamt would happen before 2020.
So don't be afraid to dream, but while you're dreaming up that perfect solution, do advocate for improved testability of the systems, starting from the very basic, with architecture.

Also, if you're really into building stuff, don't be afraid to roll your own version of off-the-shelf solutions you were already using.
The thing to take care though, is to first understand the issue you're solving thoroughly and to make sure that you really squeezed the last drop of functionality and performance out of what you were using before rolling out your own.
A couple of years ago, I thought it was cool to roll my own version of Selenium page objects without having even read about Selenium page objects. Needless to say, the end-result was a mess that is still haunting the code base üôÑ
						</aside>
					</section>
				</section>
				<section>
					<h1>Thank you!</h1>
					<aside class="notes" data-trim data-markdown>That was my story, hope you find useful.

Thank you!
					</aside>
				</section>
				<section>
					<h2>Colophon</h2>
					<p>Built with ‚ô• in <a href="https://code.visualstudio.com/">VS Code</a>, using <a href="https://lab.hakim.se/reveal">reveal.js</a></p>
					<p>Fair trade emojis sourced from <a href="http://emojipedia.org/">Emojipedia</a>; responsibly rendered using ‚ú®<a href="https://github.com/twitter/twemoji">twEmoji</a>‚ú®</p>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				backgroundTransition: 'slide',
				transition: 'slide',
				slideNumber: 'c/t',

				history: true,
				progress: true,
				overview: true,
				
				width: 800,
				height: 600,

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
			Reveal.addEventListener('ready', function(event) {
				Reveal.configure({ showNotes: Reveal.getQueryHash()['showNotes'] });
			});
			
			document.addEventListener('DOMContentLoaded', function(event) {
				twemoji.parse(document.body);
			});
		</script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-68119344-2', 'auto');ga('send', 'pageview');
</script>
	</body>
</html>
